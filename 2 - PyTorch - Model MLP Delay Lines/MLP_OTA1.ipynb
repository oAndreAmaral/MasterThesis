{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import modin.pandas as n_pd\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Datatable Import\n",
    "import datatable as dt\n",
    "\n",
    "# Deep Learning Pytorch Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchinfo import summary\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import using datatable\n",
    "X = dt.fread(\"X_MLP_DelayLine2_all_dims.csv\")\n",
    "Y = dt.fread(\"Y_MLP_DelayLine2_all_dims.csv\")\n",
    "\n",
    "# Convert to pandas\n",
    "X = X.to_pandas()\n",
    "X = X.dropna()\n",
    "Y = Y.to_pandas()\n",
    "Y = Y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60296269, 34)\n",
      "(60296269, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all dataset for train\n",
    "all_dataX = Variable(torch.Tensor(X.values)) # .cuda()\n",
    "all_dataY = Variable(torch.Tensor(Y.values)) # .cuda()\n",
    "\n",
    "# Delete X and Y from memory\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dataset\n",
    "ds = TensorDataset(all_dataX, all_dataY)\n",
    "del all_dataX, all_dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "BatchSize = 10000\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(123)\n",
    "\n",
    "# Create the DataLoader\n",
    "dl = DataLoader(ds, batch_size=BatchSize, shuffle=False, num_workers= 8, pin_memory=True, drop_last=True, generator=g) #shuffle was at false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=6, h1=200, h2=100, h3 =25, out_features=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input layer -> 1 hidden -> 2 hidden -> output \n",
    "        self.fc1 = nn.Linear(in_features,h1) # input layer (+28 to include the dimensions)\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.fc2 = nn.Linear(h1 + 28, h2)            # hidden layer\n",
    "        self.fc3 = nn.Linear(h2,h2)  # output layer\n",
    "        self.fc4 = nn.Linear(h2,h3)\n",
    "        self.fc5 = nn.Linear(h3, out_features)  # output layer\n",
    "                \n",
    "    def forward(self, x, dimensions):\n",
    "        x = F.elu(self.fc1(x))\n",
    "        in_fc = torch.cat((x.cuda(), dimensions.cuda()), dim=1).cuda()\n",
    "        x = F.elu(self.fc2(in_fc))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DataParallel                             --\n",
       "├─Model: 1-1                             --\n",
       "│    └─Linear: 2-1                       1,400\n",
       "│    └─Dropout: 2-2                      --\n",
       "│    └─Linear: 2-3                       22,900\n",
       "│    └─Linear: 2-4                       10,100\n",
       "│    └─Linear: 2-5                       2,525\n",
       "│    └─Linear: 2-6                       52\n",
       "=================================================================\n",
       "Total params: 36,977\n",
       "Trainable params: 36,977\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)\n",
    "model = Model()\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1]).cuda()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #0.0001\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Epoch: 0, ----->loss: 3.57529E-04\n",
      "Learning Rate: 1.000000E-04 \n",
      "-----> Epoch: 2, ----->loss: 3.36445E-04\n",
      "Learning Rate: 1.000000E-04 \n",
      "-----> Epoch: 4, ----->loss: 3.34465E-04\n",
      "Learning Rate: 1.000000E-04 \n",
      "-----> Epoch: 6, ----->loss: 3.30595E-04\n",
      "Learning Rate: 1.000000E-04 \n",
      "-----> Epoch: 8, ----->loss: 3.17372E-04\n",
      "Learning Rate: 1.000000E-04 \n",
      "-----> Epoch: 10, ----->loss: 3.13362E-04\n",
      "Learning Rate: 1.000000E-04 \n",
      "-----> Epoch: 12, ----->loss: 3.10381E-04\n",
      "Learning Rate: 1.000000E-04 \n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "number_of_epochs = 60\n",
    "\n",
    "# Define the start time \n",
    "start_time = time.time()\n",
    "\n",
    "# Variable to store the losses per epoch\n",
    "losses = []\n",
    "errors = []\n",
    "\n",
    "# Instanciate that the model is in training mode\n",
    "model.train()\n",
    "\n",
    "# For every epoch defined...\n",
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    # Feed data in batches\n",
    "    for batch_features, batch_labels in dl:\n",
    "        \n",
    "        # Extract input waves \n",
    "        input_waves = batch_features[:,0:6]\n",
    "        input_waves = input_waves.cuda()\n",
    "\n",
    "        # Extract the dimensions from all batch\n",
    "        input_dimensions = batch_features[:,6:(batch_features.shape[1]+1)]\n",
    "        input_dimension = input_dimensions.cuda()\n",
    "\n",
    "        # Pass batch labels to GPU\n",
    "        batch_labels = batch_labels.cuda()\n",
    "\n",
    "        # Initialize the gradient to avoid value agregations\n",
    "        optimizer.zero_grad()       \n",
    "      \n",
    "        # Predict the model using the training set\n",
    "        y_pred = model(input_waves, input_dimensions)\n",
    "\n",
    "        # Obtain the loss and the error\n",
    "        loss = criterion(y_pred, batch_labels)\n",
    "        error = y_pred - batch_labels\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1) # gradient cliping to avoid exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    # Append the losses to make the plot\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Append the errors to obtain the metrics to create the noise\n",
    "    errors.append(error)\n",
    "    \n",
    "    # Print the results\n",
    "    if epoch % 2 == 0:\n",
    "        print(\"-----> Epoch: %d, ----->loss: %.5E\" % (epoch, loss.item()))\n",
    "        print(\"Learning Rate: %.6E \" % optimizer.param_groups[0][\"lr\"])\n",
    "            \n",
    "# Print the execution time\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.0f} Min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, '/home/aamaral/Desktop/BigDataset/Evaluation/MLP_DelayLine2_Sig_all_8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e367e3eba0c249af86b9dca2e8a5cf4bb1dd64b5ae1ee422094ad095610b564"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
